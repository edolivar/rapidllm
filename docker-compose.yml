services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/usr/local/transcribe
    user: dev
    command: ["fastapi", "dev", "main.py", "--host", "0.0.0.0"]
    environment:
      - BASE_URL=http://host.docker.internal:12434/engines/llama.cpp/v1/
    models:
      - llm

models:
  llm:
    model: ai/gemma3n
